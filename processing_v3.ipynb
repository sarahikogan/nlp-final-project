{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "df = pd.read_csv('yelp.csv')\n",
    "\n",
    "# TODO remove later: sampling \n",
    "df = df.sample(n=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4712    [place, albertson, definitely, high, end, like...\n",
      "7943    [go, place, time, night, big, plus, place, par...\n",
      "3836                                     [legit, brewpub]\n",
      "6825    [find, place, accident, blink, miss, well, fry...\n",
      "6928    [place, near, yelp, say, chandler, ray, road, ...\n",
      "                              ...                        \n",
      "8256    [place, hole, wall, right, phx, car, rental, c...\n",
      "8842    [burger, tasteless, weird, texture, fry, decen...\n",
      "5945    [go, happy, hour, long, time, friend, want, tr...\n",
      "8342    [visit, high, review, think, little, rate, goo...\n",
      "6023    [eat, time, food, good, time, eat, find, bug, ...\n",
      "Name: text, Length: 500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "# TODO come back and touch up: simple processing for now\n",
    "def preprocess(text): \n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and token.text not in nlp.Defaults.stop_words]\n",
    "    return tokens\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess)\n",
    "print(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [place, albertson, definitely, high, end, like...\n",
      "1    [go, place, time, night, big, plus, place, par...\n",
      "2                                     [legit, brewpub]\n",
      "3    [find, place, accident, blink, miss, well, fry...\n",
      "4    [place, near, yelp, say, chandler, ray, road, ...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('yelp_processed_v3.csv')\n",
    "\n",
    "# turn tokens back into arrays\n",
    "import ast\n",
    "df['text'] = df['text'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "print(df['text'].head())\n",
    "\n",
    "type(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('yelp_processed_v3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Aspect Extraction\n",
    "After preprocessing, the next goal is to extract the most important topics, to find points on which we can give feedback. Below are three methods of aspect extraction. \n",
    "\n",
    "#### Initial Results (1000 rows, dataset not split into different business types)\n",
    "\n",
    "##### SpaCy: \n",
    "\n",
    "place, food, time, order, service\n",
    "\n",
    "##### LDA: \n",
    "\n",
    "Topic 0: good, place, food, order, like, time, great, chicken, love, try\n",
    "\n",
    "Topic 1: great, place, like, good, food, time, look, service, try, nice\n",
    "\n",
    "Topic 2: good, like, time, great, food, place, come, order, service, restaurant\n",
    "\n",
    "Topic 3: good, place, like, food, time, service, price, come, way, great\n",
    "\n",
    "Topic 4: like, place, great, burger, food, good, drink, want, love, think\n",
    "\n",
    "#### TF-IDF: \n",
    "\n",
    "**Without sentiments:** food, time, service, come, go\n",
    "\n",
    "**With only nouns:** food, place, time, service, order\n",
    "\n",
    "\n",
    "TODO: try LDA without sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Aspect Extraction\n",
    "This code extracts the aspects (nouns and adj-noun pairs) from each of the sentences, then finds the most common aspects in the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('place', 663), ('food', 545), ('time', 451), ('order', 355), ('service', 302), ('love', 216), ('restaurant', 210), ('price', 196), ('thing', 180), ('people', 161), ('day', 157), ('staff', 151), ('night', 148), ('experience', 143), ('table', 143), ('burger', 140), ('way', 139), ('friend', 139), ('lot', 138), ('hour', 137)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "# TODO categorize by type of building\n",
    "def extract_aspects(text):\n",
    "    doc = nlp(\" \".join(text))\n",
    "    aspects = [token.text for token in doc if token.pos_ == \"NOUN\"] # extract nouns\n",
    "    adj_noun_pairs = [\" \".join([token.text, token.head.text]) for token in doc if token.pos_ == \"ADJ\" and token.head.pos_ == \"NOUN\"] # extract important pairs\n",
    "    return aspects + adj_noun_pairs\n",
    "\n",
    "df['aspects'] = df['text'].apply(extract_aspects)\n",
    "\n",
    "# count common aspects\n",
    "all_aspects = [aspect for sublist in df['aspects'] for aspect in sublist ]\n",
    "aspect_counts = Counter(all_aspects)\n",
    "print(aspect_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    "LDA assigns probability distributions over topics to each document, and produces the top related words for each topic. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: good, place, food, order, like, time, great, chicken, love, try\n",
      "Topic 1: great, place, like, good, food, time, look, service, try, nice\n",
      "Topic 2: good, like, time, great, food, place, come, order, service, restaurant\n",
      "Topic 3: good, place, like, food, time, service, price, come, way, great\n",
      "Topic 4: like, place, great, burger, food, good, drink, want, love, think\n"
     ]
    }
   ],
   "source": [
    "# trying LDA \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "X = vectorizer.fit_transform([\" \".join(words) for words in df['text']])\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "# get top words per topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_): \n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
    "    print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Compute TF-IDF scores for each word, then select top words with highest scores as aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('yelp_processed_v3.csv')\n",
    "\n",
    "# turn tokens back into arrays\n",
    "import ast\n",
    "df['text'] = df['text'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "if (type(df['text'][0] != 'text')): df['text'] = df['text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Processing without sentiment words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    albertson definitely high end food barrel clea...\n",
      "1    go time night big plus parking check sure rest...\n",
      "2                                        legit brewpub\n",
      "3    find accident blink miss well fry catfish tota...\n",
      "4      near yelp say chandler ray road industrial area\n",
      "5    girls want splurge go hair salon time money cl...\n",
      "6    big organic kind guy care hormone free bpa fre...\n",
      "7    go get phoenix go month mom favorite eat under...\n",
      "8    ok stingray high point trendy hip explode youn...\n",
      "9    food northern chinese similar chinese muslim f...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# remove sentiment words (as possible) so the data doesn't get skewed \n",
    "sentiment_words = ['place', 'great', 'good', 'excellent', 'bad', 'terrible', 'like', 'nice', 'love', 'hate', 'amazing', 'wonderful', 'fantastic']\n",
    "def remove_sentiment(text, sentiment_words): \n",
    "    return \" \".join([word for word in text.split() if word.lower() not in sentiment_words])\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: remove_sentiment(x, sentiment_words))\n",
    "print(df['text'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Processing with only nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate processing, filtered for nouns \n",
    "\n",
    "def filter_nouns(text): \n",
    "    doc = nlp(text)\n",
    "    nouns = [token.text for token in doc if token.pos_ in ['NOUN']]\n",
    "    return \" \".join(nouns)\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: filter_nouns(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('food', 0.06452233170841527), ('place', 0.05299159837590725), ('time', 0.03751332767274223), ('service', 0.03545812582771143), ('order', 0.026482994183309364), ('restaurant', 0.0231678198924396), ('love', 0.0220315312477425), ('price', 0.02188395824593481), ('staff', 0.02152927352995993), ('bar', 0.01960026585892079), ('burger', 0.01847028556501228), ('hour', 0.017986373129966933), ('day', 0.017277618719814026), ('drink', 0.01694587674196936), ('thing', 0.016554321190996587), ('experience', 0.0164796254543083), ('night', 0.016339683684711954), ('people', 0.016071050222510867), ('meal', 0.015739915332787225), ('taste', 0.01565029664736228)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# get average tfidf scores \n",
    "tfidf_scores = tfidf_matrix.mean(axis=0).A1\n",
    "word_scores = dict(zip(feature_names, tfidf_scores))\n",
    "\n",
    "# top 20 words \n",
    "aspects = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "print(aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: ABSA (Aspect-Based Sentiment Analysis)\n",
    "\n",
    "The next step is to analyze each review in relation to each of these aspects. \n",
    "\n",
    "1. Extract aspect-related content from each review. \n",
    "\n",
    "2. Compute the sentiment for each aspect for each review (positive, negative, neutral). If there is no data in a review about an aspect, it is classified as Neutral. \n",
    "\n",
    "\n",
    "TODO: expand number of aspects, only 3 are used now for efficiency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset dataset back to pre-processing, before tokenization\n",
    "df = pd.read_csv('yelp_processed_v3.csv')\n",
    "\n",
    "# turn tokens back into arrays\n",
    "import ast\n",
    "df['text'] = df['text'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "if (type(df['text'][0] != 'text')): df['text'] = df['text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign sentiment to each aspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ABSA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embeddings for review text\n",
    "\n",
    "# train classification model (try different ones: logreg for tfidf, rnns, bert)\n",
    "# multi-label classification model \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
